{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11u0t38-zVkBi-CZU3SDvb3daNmrdKJqw","timestamp":1730429775742}],"authorship_tag":"ABX9TyOaJ8CHqmkyF/Pj4avjJc3L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":66,"metadata":{"id":"cUCAasycNUxq","executionInfo":{"status":"ok","timestamp":1730428223913,"user_tz":240,"elapsed":278,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}}},"outputs":[],"source":["from torchvision import datasets\n","from torchvision.transforms import ToTensor"]},{"cell_type":"code","source":["train_data = datasets.MNIST(\n","    root = 'data',\n","    train = True,\n","    transform = ToTensor(),\n","    download = True\n",")\n","\n","test_data = datasets.MNIST(\n","    root = 'data',\n","    train = False,\n","    transform = ToTensor(),\n","    download = True\n",")"],"metadata":{"id":"Qla8tz2nOR99","executionInfo":{"status":"ok","timestamp":1730428231571,"user_tz":240,"elapsed":1017,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["train_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoKrJ7tfPSVR","executionInfo":{"status":"ok","timestamp":1730428232431,"user_tz":240,"elapsed":5,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"56ad432a-3fe7-4576-b17a-dd515f787be0"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset MNIST\n","    Number of datapoints: 60000\n","    Root location: data\n","    Split: Train\n","    StandardTransform\n","Transform: ToTensor()"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["test_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DOQ98VXPYH7","executionInfo":{"status":"ok","timestamp":1730428234536,"user_tz":240,"elapsed":1010,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"f2734dd1-6420-4c7d-941f-ce439edbffdf"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset MNIST\n","    Number of datapoints: 10000\n","    Root location: data\n","    Split: Test\n","    StandardTransform\n","Transform: ToTensor()"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["train_data.data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8oaJ_nuPPcw3","executionInfo":{"status":"ok","timestamp":1730428238826,"user_tz":240,"elapsed":270,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"8ae087ee-4a83-485f-c075-2ee374c7c701"},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]],\n","\n","        [[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]],\n","\n","        [[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]],\n","\n","        ...,\n","\n","        [[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]],\n","\n","        [[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]],\n","\n","        [[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["train_data.data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JddQ8LQ0PiL4","executionInfo":{"status":"ok","timestamp":1730428242592,"user_tz":240,"elapsed":286,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"00ce1784-0f7b-47ff-aac8-332f777b1b7e"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([60000, 28, 28])"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["test_data.data.shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9yDgw2GP1yL","executionInfo":{"status":"ok","timestamp":1730428243995,"user_tz":240,"elapsed":2,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"ec74f2c0-fbb9-44cf-de40-4e4ede8e85e8"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10000, 28, 28])"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["train_data.targets.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xG482Z37P9HM","executionInfo":{"status":"ok","timestamp":1730428244924,"user_tz":240,"elapsed":3,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"fa519e15-4fac-440a-f453-63690688ae97"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([60000])"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["train_data.targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M61YdRHnQHel","executionInfo":{"status":"ok","timestamp":1730428246011,"user_tz":240,"elapsed":3,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"20eab068-1668-42af-cba5-d6d933352a13"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5, 0, 4,  ..., 5, 6, 8])"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","loaders = {\n","    'train': DataLoader(train_data,\n","                        batch_size = 100,\n","                        shuffle=True,\n","                        num_workers=1),\n","\n","    'test': DataLoader(test_data,\n","                        batch_size = 100,\n","                        shuffle=True,\n","                        num_workers=1),\n","}"],"metadata":{"id":"YmvzF0PvQP48","executionInfo":{"status":"ok","timestamp":1730429166990,"user_tz":240,"elapsed":838,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["loaders"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPMkkrNVQ-e1","executionInfo":{"status":"ok","timestamp":1730429171823,"user_tz":240,"elapsed":295,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"65b8acf3-276f-4a2e-927e-9a20bbb9f96e"},"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'train': <torch.utils.data.dataloader.DataLoader at 0x791becfb4580>,\n"," 'test': <torch.utils.data.dataloader.DataLoader at 0x791becfb47f0>}"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.conv2_drop = nn.Dropout2d()\n","        self.fc1 = nn.Linear(320, 50)  # Make sure 320 is the correct input size\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","        x = x.view(-1, 320)  # Ensure this matches the flattened output size\n","        x = F.relu(self.fc1(x))  # Corrected from self.fcl to self.fc1\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","\n","        return F.softmax(x, dim=1)  # Corrected softMax to softmax and added dim\n","\n","    def predict(self, x):\n","        self.eval()  # Set the model to evaluation mode\n","        with torch.no_grad():  # Disable gradient tracking\n","            outputs = self(x)  # Get model outputs\n","            _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n","\n","        return predicted  # Return the predicted class labels\n","\n","\n"],"metadata":{"id":"xEBF-VDYRUmr","executionInfo":{"status":"ok","timestamp":1730429174099,"user_tz":240,"elapsed":647,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = CNN().to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def train(epoch):\n","  model.train()\n","  for batch_idx, (data, target) in enumerate(loaders['train']):\n","    data, target = data.to(device), target.to(device)\n","    optimizer.zero_grad()\n","    output = model(data)\n","    loss = loss_fn(output, target)\n","    loss.backward()\n","    optimizer.step()\n","    if batch_idx %  20 == 0:\n","      print(f'Train Epoch: {epoch}  [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} '\n","      f'({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n","\n","def test():\n","  model.eval()\n","\n","  test_loss = 0\n","  correct = 0\n","\n","\n","  with torch.no_grad():\n","    for data, target in loaders['test']:\n","      data, target = data.to(device), target.to(device)\n","      output = model(data)\n","      test_loss += loss_fn(output, target).item()\n","      pred = output.argmax(dim=1, keepdim = True)\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","      test_loss /= len(loaders['test'].dataset)\n","      print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} '\n","      f'({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')\n","\n","\n","\n","\n"],"metadata":{"id":"PMD7zTLtTdJD","executionInfo":{"status":"ok","timestamp":1730429178940,"user_tz":240,"elapsed":812,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, 11):\n","  train(epoch)\n","  test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yxdtbkc3WxOx","executionInfo":{"status":"ok","timestamp":1730429464215,"user_tz":240,"elapsed":279871,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"05a89be4-1f53-41bf-8af3-54f669736c4d"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1  [0/60000 (0%)]\t2.300724\n","Train Epoch: 1  [2000/60000 (3%)]\t2.299034\n","Train Epoch: 1  [4000/60000 (7%)]\t2.187617\n","Train Epoch: 1  [6000/60000 (10%)]\t2.041675\n","Train Epoch: 1  [8000/60000 (13%)]\t1.843549\n","Train Epoch: 1  [10000/60000 (17%)]\t1.802261\n","Train Epoch: 1  [12000/60000 (20%)]\t1.795683\n","Train Epoch: 1  [14000/60000 (23%)]\t1.753202\n","Train Epoch: 1  [16000/60000 (27%)]\t1.809084\n","Train Epoch: 1  [18000/60000 (30%)]\t1.735932\n","Train Epoch: 1  [20000/60000 (33%)]\t1.681515\n","Train Epoch: 1  [22000/60000 (37%)]\t1.667239\n","Train Epoch: 1  [24000/60000 (40%)]\t1.705417\n","Train Epoch: 1  [26000/60000 (43%)]\t1.674333\n","Train Epoch: 1  [28000/60000 (47%)]\t1.645470\n","Train Epoch: 1  [30000/60000 (50%)]\t1.698159\n","Train Epoch: 1  [32000/60000 (53%)]\t1.684569\n","Train Epoch: 1  [34000/60000 (57%)]\t1.642772\n","Train Epoch: 1  [36000/60000 (60%)]\t1.646816\n","Train Epoch: 1  [38000/60000 (63%)]\t1.683777\n","Train Epoch: 1  [40000/60000 (67%)]\t1.678245\n","Train Epoch: 1  [42000/60000 (70%)]\t1.641370\n","Train Epoch: 1  [44000/60000 (73%)]\t1.632692\n","Train Epoch: 1  [46000/60000 (77%)]\t1.637069\n","Train Epoch: 1  [48000/60000 (80%)]\t1.641949\n","Train Epoch: 1  [50000/60000 (83%)]\t1.583905\n","Train Epoch: 1  [52000/60000 (87%)]\t1.604038\n","Train Epoch: 1  [54000/60000 (90%)]\t1.623855\n","Train Epoch: 1  [56000/60000 (93%)]\t1.638885\n","Train Epoch: 1  [58000/60000 (97%)]\t1.631140\n","\n","Test set: Average loss: 0.0002, Accuracy: 95/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 191/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 287/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 378/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 471/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 565/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 660/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 756/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 848/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 944/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1038/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1132/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1228/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1321/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1415/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1510/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1602/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1697/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1789/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1886/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1980/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2074/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2165/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2258/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2354/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2446/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2539/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2633/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2726/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2823/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2920/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3010/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3104/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3195/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3291/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3387/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3481/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3577/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3674/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3765/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3861/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3959/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4055/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4145/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4237/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4334/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4424/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4518/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4613/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4705/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4797/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4893/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4985/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5076/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5167/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5257/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5350/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5443/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5539/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5636/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5731/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5828/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5921/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6013/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6106/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6196/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6292/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6382/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6474/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6562/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6652/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6743/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6838/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6933/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7025/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7118/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7210/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7300/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7394/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7487/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7585/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7675/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7767/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7858/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7956/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8051/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8147/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8245/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8341/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8435/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8530/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8623/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8715/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8807/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8900/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8993/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9088/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9183/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9278/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9371/10000 (94%)\n","\n","Train Epoch: 2  [0/60000 (0%)]\t1.545896\n","Train Epoch: 2  [2000/60000 (3%)]\t1.627621\n","Train Epoch: 2  [4000/60000 (7%)]\t1.602030\n","Train Epoch: 2  [6000/60000 (10%)]\t1.617191\n","Train Epoch: 2  [8000/60000 (13%)]\t1.616004\n","Train Epoch: 2  [10000/60000 (17%)]\t1.696715\n","Train Epoch: 2  [12000/60000 (20%)]\t1.574536\n","Train Epoch: 2  [14000/60000 (23%)]\t1.514382\n","Train Epoch: 2  [16000/60000 (27%)]\t1.569099\n","Train Epoch: 2  [18000/60000 (30%)]\t1.569090\n","Train Epoch: 2  [20000/60000 (33%)]\t1.594292\n","Train Epoch: 2  [22000/60000 (37%)]\t1.598572\n","Train Epoch: 2  [24000/60000 (40%)]\t1.581382\n","Train Epoch: 2  [26000/60000 (43%)]\t1.541948\n","Train Epoch: 2  [28000/60000 (47%)]\t1.580077\n","Train Epoch: 2  [30000/60000 (50%)]\t1.552154\n","Train Epoch: 2  [32000/60000 (53%)]\t1.621181\n","Train Epoch: 2  [34000/60000 (57%)]\t1.505912\n","Train Epoch: 2  [36000/60000 (60%)]\t1.555713\n","Train Epoch: 2  [38000/60000 (63%)]\t1.577378\n","Train Epoch: 2  [40000/60000 (67%)]\t1.632698\n","Train Epoch: 2  [42000/60000 (70%)]\t1.559699\n","Train Epoch: 2  [44000/60000 (73%)]\t1.563340\n","Train Epoch: 2  [46000/60000 (77%)]\t1.585270\n","Train Epoch: 2  [48000/60000 (80%)]\t1.582668\n","Train Epoch: 2  [50000/60000 (83%)]\t1.615594\n","Train Epoch: 2  [52000/60000 (87%)]\t1.568205\n","Train Epoch: 2  [54000/60000 (90%)]\t1.615674\n","Train Epoch: 2  [56000/60000 (93%)]\t1.557202\n","Train Epoch: 2  [58000/60000 (97%)]\t1.545495\n","\n","Test set: Average loss: 0.0002, Accuracy: 96/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 194/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 289/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 387/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 486/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 580/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 676/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 773/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 869/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 965/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1062/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1154/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1253/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1350/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1446/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1541/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1634/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1730/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1827/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1924/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2017/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2114/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2214/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2312/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2408/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2501/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2593/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2689/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2782/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2877/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2974/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3069/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3166/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3261/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3354/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3449/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3546/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3639/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3735/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3828/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3924/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4019/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4115/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4211/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4307/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4399/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4496/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4593/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4687/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4782/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4877/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4972/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5066/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5159/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5253/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5352/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5447/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5544/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5641/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5737/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5834/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5928/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6024/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6120/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6217/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6310/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6407/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6502/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6598/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6694/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6789/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6882/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6973/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7068/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7166/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7259/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7354/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7447/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7540/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7636/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7728/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7823/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7921/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8016/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8111/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8205/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8298/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8393/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8489/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8585/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8681/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8777/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8875/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8970/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9060/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9158/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9253/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9347/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9442/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9537/10000 (95%)\n","\n","Train Epoch: 3  [0/60000 (0%)]\t1.529289\n","Train Epoch: 3  [2000/60000 (3%)]\t1.531399\n","Train Epoch: 3  [4000/60000 (7%)]\t1.540329\n","Train Epoch: 3  [6000/60000 (10%)]\t1.509644\n","Train Epoch: 3  [8000/60000 (13%)]\t1.536593\n","Train Epoch: 3  [10000/60000 (17%)]\t1.586852\n","Train Epoch: 3  [12000/60000 (20%)]\t1.571646\n","Train Epoch: 3  [14000/60000 (23%)]\t1.574971\n","Train Epoch: 3  [16000/60000 (27%)]\t1.539172\n","Train Epoch: 3  [18000/60000 (30%)]\t1.542801\n","Train Epoch: 3  [20000/60000 (33%)]\t1.640152\n","Train Epoch: 3  [22000/60000 (37%)]\t1.570460\n","Train Epoch: 3  [24000/60000 (40%)]\t1.542293\n","Train Epoch: 3  [26000/60000 (43%)]\t1.564818\n","Train Epoch: 3  [28000/60000 (47%)]\t1.533525\n","Train Epoch: 3  [30000/60000 (50%)]\t1.510780\n","Train Epoch: 3  [32000/60000 (53%)]\t1.534964\n","Train Epoch: 3  [34000/60000 (57%)]\t1.543162\n","Train Epoch: 3  [36000/60000 (60%)]\t1.538573\n","Train Epoch: 3  [38000/60000 (63%)]\t1.536600\n","Train Epoch: 3  [40000/60000 (67%)]\t1.535697\n","Train Epoch: 3  [42000/60000 (70%)]\t1.575549\n","Train Epoch: 3  [44000/60000 (73%)]\t1.534733\n","Train Epoch: 3  [46000/60000 (77%)]\t1.546093\n","Train Epoch: 3  [48000/60000 (80%)]\t1.507294\n","Train Epoch: 3  [50000/60000 (83%)]\t1.582325\n","Train Epoch: 3  [52000/60000 (87%)]\t1.539659\n","Train Epoch: 3  [54000/60000 (90%)]\t1.526149\n","Train Epoch: 3  [56000/60000 (93%)]\t1.560976\n","Train Epoch: 3  [58000/60000 (97%)]\t1.573407\n","\n","Test set: Average loss: 0.0001, Accuracy: 98/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 194/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 289/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 385/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 476/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 572/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 662/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 760/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 858/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 956/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1053/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1146/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1244/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1338/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1429/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1523/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1620/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1717/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1813/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1909/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2005/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2101/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2198/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2294/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2392/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2491/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2588/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2686/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2782/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2876/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2974/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3069/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3166/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3263/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3358/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3451/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3545/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3640/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3737/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3834/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3931/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4024/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4119/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4216/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4314/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4409/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4506/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4601/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4697/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4797/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4896/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4995/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5092/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5187/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5286/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5380/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5472/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5572/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5666/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5763/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5857/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5953/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6052/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6150/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6248/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6344/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6440/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6538/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6637/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6730/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6827/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6922/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7019/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7116/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7206/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7302/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7400/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7494/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7593/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7687/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7782/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7880/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7972/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8070/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8166/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8261/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8357/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8454/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8551/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8650/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8744/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8842/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8939/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9035/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9132/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9229/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9324/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9422/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9519/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9616/10000 (96%)\n","\n","Train Epoch: 4  [0/60000 (0%)]\t1.561430\n","Train Epoch: 4  [2000/60000 (3%)]\t1.598125\n","Train Epoch: 4  [4000/60000 (7%)]\t1.585368\n","Train Epoch: 4  [6000/60000 (10%)]\t1.548312\n","Train Epoch: 4  [8000/60000 (13%)]\t1.540205\n","Train Epoch: 4  [10000/60000 (17%)]\t1.582266\n","Train Epoch: 4  [12000/60000 (20%)]\t1.563820\n","Train Epoch: 4  [14000/60000 (23%)]\t1.520462\n","Train Epoch: 4  [16000/60000 (27%)]\t1.506073\n","Train Epoch: 4  [18000/60000 (30%)]\t1.552641\n","Train Epoch: 4  [20000/60000 (33%)]\t1.522239\n","Train Epoch: 4  [22000/60000 (37%)]\t1.531229\n","Train Epoch: 4  [24000/60000 (40%)]\t1.526454\n","Train Epoch: 4  [26000/60000 (43%)]\t1.489676\n","Train Epoch: 4  [28000/60000 (47%)]\t1.553509\n","Train Epoch: 4  [30000/60000 (50%)]\t1.501966\n","Train Epoch: 4  [32000/60000 (53%)]\t1.531768\n","Train Epoch: 4  [34000/60000 (57%)]\t1.600959\n","Train Epoch: 4  [36000/60000 (60%)]\t1.534947\n","Train Epoch: 4  [38000/60000 (63%)]\t1.551108\n","Train Epoch: 4  [40000/60000 (67%)]\t1.511927\n","Train Epoch: 4  [42000/60000 (70%)]\t1.506873\n","Train Epoch: 4  [44000/60000 (73%)]\t1.543809\n","Train Epoch: 4  [46000/60000 (77%)]\t1.555193\n","Train Epoch: 4  [48000/60000 (80%)]\t1.580132\n","Train Epoch: 4  [50000/60000 (83%)]\t1.522981\n","Train Epoch: 4  [52000/60000 (87%)]\t1.507984\n","Train Epoch: 4  [54000/60000 (90%)]\t1.547844\n","Train Epoch: 4  [56000/60000 (93%)]\t1.546091\n","Train Epoch: 4  [58000/60000 (97%)]\t1.585315\n","\n","Test set: Average loss: 0.0002, Accuracy: 96/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 195/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 289/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 388/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 483/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 581/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 676/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 772/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 868/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 965/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1064/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1161/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1256/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1351/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1440/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1534/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1630/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1729/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1828/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1925/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2020/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2118/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2213/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2311/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2408/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2504/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2600/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2693/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2792/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2888/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2984/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3082/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3179/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3275/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3373/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3467/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3565/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3662/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3759/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3855/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3950/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4047/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4146/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4239/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4337/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4434/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4533/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4631/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4730/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4826/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4921/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5016/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5113/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5209/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5307/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5399/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5497/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5595/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5689/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5785/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5881/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5980/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6079/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6172/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6271/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6368/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6463/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6559/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6659/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6758/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6853/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6948/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7045/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7143/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7243/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7338/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7434/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7531/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7627/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7725/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7823/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7920/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8017/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8116/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8214/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8311/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8408/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8507/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8602/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8699/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8793/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8891/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8986/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9080/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9174/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9271/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9370/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9467/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9559/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9657/10000 (97%)\n","\n","Train Epoch: 5  [0/60000 (0%)]\t1.563514\n","Train Epoch: 5  [2000/60000 (3%)]\t1.561025\n","Train Epoch: 5  [4000/60000 (7%)]\t1.554726\n","Train Epoch: 5  [6000/60000 (10%)]\t1.573800\n","Train Epoch: 5  [8000/60000 (13%)]\t1.517013\n","Train Epoch: 5  [10000/60000 (17%)]\t1.546184\n","Train Epoch: 5  [12000/60000 (20%)]\t1.506244\n","Train Epoch: 5  [14000/60000 (23%)]\t1.611747\n","Train Epoch: 5  [16000/60000 (27%)]\t1.525120\n","Train Epoch: 5  [18000/60000 (30%)]\t1.575727\n","Train Epoch: 5  [20000/60000 (33%)]\t1.558776\n","Train Epoch: 5  [22000/60000 (37%)]\t1.527697\n","Train Epoch: 5  [24000/60000 (40%)]\t1.547154\n","Train Epoch: 5  [26000/60000 (43%)]\t1.492878\n","Train Epoch: 5  [28000/60000 (47%)]\t1.528633\n","Train Epoch: 5  [30000/60000 (50%)]\t1.581039\n","Train Epoch: 5  [32000/60000 (53%)]\t1.550876\n","Train Epoch: 5  [34000/60000 (57%)]\t1.526885\n","Train Epoch: 5  [36000/60000 (60%)]\t1.513564\n","Train Epoch: 5  [38000/60000 (63%)]\t1.565950\n","Train Epoch: 5  [40000/60000 (67%)]\t1.541127\n","Train Epoch: 5  [42000/60000 (70%)]\t1.494225\n","Train Epoch: 5  [44000/60000 (73%)]\t1.515808\n","Train Epoch: 5  [46000/60000 (77%)]\t1.500844\n","Train Epoch: 5  [48000/60000 (80%)]\t1.572508\n","Train Epoch: 5  [50000/60000 (83%)]\t1.533801\n","Train Epoch: 5  [52000/60000 (87%)]\t1.528545\n","Train Epoch: 5  [54000/60000 (90%)]\t1.527179\n","Train Epoch: 5  [56000/60000 (93%)]\t1.536645\n","Train Epoch: 5  [58000/60000 (97%)]\t1.570772\n","\n","Test set: Average loss: 0.0002, Accuracy: 95/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 190/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 284/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 379/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 475/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 570/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 667/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 765/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 863/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 960/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1057/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1154/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1253/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1349/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1445/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1540/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1636/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1735/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1832/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1929/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2026/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2123/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2220/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2318/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2414/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2512/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2608/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2706/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2803/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2899/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2993/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3090/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3187/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3284/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3382/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3479/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3575/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3674/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3771/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3868/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3966/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4064/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4160/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4256/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4353/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4451/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4548/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4644/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4742/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4840/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4937/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5032/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5130/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5227/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5325/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5420/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5517/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5617/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5712/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5810/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5910/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6004/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6099/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6199/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6294/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6389/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6487/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6584/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6682/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6778/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6874/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6968/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7064/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7162/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7260/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7357/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7449/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7546/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7639/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7736/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7833/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7931/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8028/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8124/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8220/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8313/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8406/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8500/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8594/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8692/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8791/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8886/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8984/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9084/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9180/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9276/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9374/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9474/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9572/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9665/10000 (97%)\n","\n","Train Epoch: 6  [0/60000 (0%)]\t1.551952\n","Train Epoch: 6  [2000/60000 (3%)]\t1.496881\n","Train Epoch: 6  [4000/60000 (7%)]\t1.574481\n","Train Epoch: 6  [6000/60000 (10%)]\t1.534952\n","Train Epoch: 6  [8000/60000 (13%)]\t1.473376\n","Train Epoch: 6  [10000/60000 (17%)]\t1.522102\n","Train Epoch: 6  [12000/60000 (20%)]\t1.512851\n","Train Epoch: 6  [14000/60000 (23%)]\t1.527877\n","Train Epoch: 6  [16000/60000 (27%)]\t1.548423\n","Train Epoch: 6  [18000/60000 (30%)]\t1.522757\n","Train Epoch: 6  [20000/60000 (33%)]\t1.515028\n","Train Epoch: 6  [22000/60000 (37%)]\t1.518664\n","Train Epoch: 6  [24000/60000 (40%)]\t1.536037\n","Train Epoch: 6  [26000/60000 (43%)]\t1.505498\n","Train Epoch: 6  [28000/60000 (47%)]\t1.590416\n","Train Epoch: 6  [30000/60000 (50%)]\t1.532763\n","Train Epoch: 6  [32000/60000 (53%)]\t1.537489\n","Train Epoch: 6  [34000/60000 (57%)]\t1.539333\n","Train Epoch: 6  [36000/60000 (60%)]\t1.563696\n","Train Epoch: 6  [38000/60000 (63%)]\t1.552278\n","Train Epoch: 6  [40000/60000 (67%)]\t1.527483\n","Train Epoch: 6  [42000/60000 (70%)]\t1.554823\n","Train Epoch: 6  [44000/60000 (73%)]\t1.534697\n","Train Epoch: 6  [46000/60000 (77%)]\t1.538705\n","Train Epoch: 6  [48000/60000 (80%)]\t1.508390\n","Train Epoch: 6  [50000/60000 (83%)]\t1.510001\n","Train Epoch: 6  [52000/60000 (87%)]\t1.481231\n","Train Epoch: 6  [54000/60000 (90%)]\t1.487571\n","Train Epoch: 6  [56000/60000 (93%)]\t1.522899\n","Train Epoch: 6  [58000/60000 (97%)]\t1.542320\n","\n","Test set: Average loss: 0.0001, Accuracy: 99/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 198/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 295/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 390/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 483/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 581/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 680/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 773/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 868/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 966/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1065/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1162/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1260/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1358/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1455/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1550/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1645/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1741/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1839/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1934/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2030/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2126/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2221/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2319/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2415/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2514/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2613/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2711/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2811/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2906/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3003/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3100/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3196/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3296/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3391/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3489/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3586/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3684/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3779/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3876/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3973/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4072/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4167/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4265/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4364/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4461/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4555/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4651/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4750/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4850/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4947/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5043/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5141/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5239/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5336/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5432/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5527/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5626/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5723/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5818/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5914/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6011/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6107/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6206/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6301/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6397/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6496/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6594/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6692/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6789/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6887/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6980/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7080/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7174/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7270/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7367/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7466/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7562/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7660/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7757/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7857/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7949/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8045/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8141/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8240/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8337/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8435/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8535/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8631/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8730/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8826/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8923/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9023/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9120/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9216/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9313/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9410/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9510/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9609/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9705/10000 (97%)\n","\n","Train Epoch: 7  [0/60000 (0%)]\t1.540647\n","Train Epoch: 7  [2000/60000 (3%)]\t1.523574\n","Train Epoch: 7  [4000/60000 (7%)]\t1.541879\n","Train Epoch: 7  [6000/60000 (10%)]\t1.538311\n","Train Epoch: 7  [8000/60000 (13%)]\t1.572761\n","Train Epoch: 7  [10000/60000 (17%)]\t1.546404\n","Train Epoch: 7  [12000/60000 (20%)]\t1.552942\n","Train Epoch: 7  [14000/60000 (23%)]\t1.492188\n","Train Epoch: 7  [16000/60000 (27%)]\t1.558204\n","Train Epoch: 7  [18000/60000 (30%)]\t1.496621\n","Train Epoch: 7  [20000/60000 (33%)]\t1.513313\n","Train Epoch: 7  [22000/60000 (37%)]\t1.519928\n","Train Epoch: 7  [24000/60000 (40%)]\t1.523911\n","Train Epoch: 7  [26000/60000 (43%)]\t1.516293\n","Train Epoch: 7  [28000/60000 (47%)]\t1.557665\n","Train Epoch: 7  [30000/60000 (50%)]\t1.511471\n","Train Epoch: 7  [32000/60000 (53%)]\t1.534727\n","Train Epoch: 7  [34000/60000 (57%)]\t1.508619\n","Train Epoch: 7  [36000/60000 (60%)]\t1.551244\n","Train Epoch: 7  [38000/60000 (63%)]\t1.538060\n","Train Epoch: 7  [40000/60000 (67%)]\t1.499308\n","Train Epoch: 7  [42000/60000 (70%)]\t1.501015\n","Train Epoch: 7  [44000/60000 (73%)]\t1.539262\n","Train Epoch: 7  [46000/60000 (77%)]\t1.509573\n","Train Epoch: 7  [48000/60000 (80%)]\t1.512368\n","Train Epoch: 7  [50000/60000 (83%)]\t1.522119\n","Train Epoch: 7  [52000/60000 (87%)]\t1.528844\n","Train Epoch: 7  [54000/60000 (90%)]\t1.502180\n","Train Epoch: 7  [56000/60000 (93%)]\t1.498342\n","Train Epoch: 7  [58000/60000 (97%)]\t1.544135\n","\n","Test set: Average loss: 0.0001, Accuracy: 97/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 195/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 291/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 389/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 488/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 584/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 681/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 776/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 873/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 973/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1070/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1167/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1264/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1362/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1459/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1556/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1653/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1751/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1850/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1948/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2045/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2142/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2239/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2337/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2436/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2533/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2630/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2726/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2822/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2920/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3017/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3113/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3210/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3305/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3403/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3499/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3593/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3692/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3789/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3888/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3985/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4082/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4181/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4278/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4374/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4473/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4571/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4669/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4767/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4864/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4963/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5061/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5159/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5258/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5354/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5452/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5551/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5645/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5740/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5839/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5936/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6034/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6131/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6224/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6319/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6418/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6516/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6613/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6710/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6809/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6908/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7006/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7105/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7201/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7298/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7396/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7496/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7592/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7688/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7788/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7886/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7982/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8079/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8177/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8275/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8373/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8468/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8563/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8659/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8757/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8857/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8954/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9052/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9150/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9249/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9344/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9441/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9539/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9635/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9734/10000 (97%)\n","\n","Train Epoch: 8  [0/60000 (0%)]\t1.493013\n","Train Epoch: 8  [2000/60000 (3%)]\t1.522640\n","Train Epoch: 8  [4000/60000 (7%)]\t1.574543\n","Train Epoch: 8  [6000/60000 (10%)]\t1.537856\n","Train Epoch: 8  [8000/60000 (13%)]\t1.547872\n","Train Epoch: 8  [10000/60000 (17%)]\t1.513060\n","Train Epoch: 8  [12000/60000 (20%)]\t1.516816\n","Train Epoch: 8  [14000/60000 (23%)]\t1.521469\n","Train Epoch: 8  [16000/60000 (27%)]\t1.566599\n","Train Epoch: 8  [18000/60000 (30%)]\t1.521432\n","Train Epoch: 8  [20000/60000 (33%)]\t1.541942\n","Train Epoch: 8  [22000/60000 (37%)]\t1.523686\n","Train Epoch: 8  [24000/60000 (40%)]\t1.556635\n","Train Epoch: 8  [26000/60000 (43%)]\t1.516497\n","Train Epoch: 8  [28000/60000 (47%)]\t1.491035\n","Train Epoch: 8  [30000/60000 (50%)]\t1.530266\n","Train Epoch: 8  [32000/60000 (53%)]\t1.509817\n","Train Epoch: 8  [34000/60000 (57%)]\t1.537931\n","Train Epoch: 8  [36000/60000 (60%)]\t1.524351\n","Train Epoch: 8  [38000/60000 (63%)]\t1.525589\n","Train Epoch: 8  [40000/60000 (67%)]\t1.528035\n","Train Epoch: 8  [42000/60000 (70%)]\t1.527303\n","Train Epoch: 8  [44000/60000 (73%)]\t1.529527\n","Train Epoch: 8  [46000/60000 (77%)]\t1.539909\n","Train Epoch: 8  [48000/60000 (80%)]\t1.536387\n","Train Epoch: 8  [50000/60000 (83%)]\t1.526524\n","Train Epoch: 8  [52000/60000 (87%)]\t1.564502\n","Train Epoch: 8  [54000/60000 (90%)]\t1.501608\n","Train Epoch: 8  [56000/60000 (93%)]\t1.530477\n","Train Epoch: 8  [58000/60000 (97%)]\t1.554954\n","\n","Test set: Average loss: 0.0001, Accuracy: 98/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 196/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 294/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 391/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 490/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 588/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 682/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 781/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 878/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 973/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1069/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1166/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1264/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1364/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1459/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1556/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1655/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1753/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1849/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1945/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2041/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2140/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2239/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2335/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2429/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2526/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2624/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2721/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2818/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2916/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3013/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3112/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3207/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3305/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3403/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3499/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3597/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3695/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3792/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3892/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3988/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4087/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4183/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4280/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4378/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4474/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4573/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4670/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4766/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4865/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4963/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5058/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5158/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5253/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5352/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5447/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5544/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5642/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5740/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5837/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5936/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6034/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6132/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6230/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6327/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6424/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6521/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6621/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6720/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6818/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6914/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7014/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7111/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7209/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7305/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7400/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7499/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7596/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7695/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7792/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7890/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7986/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8084/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8184/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8283/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8380/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8479/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8574/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8673/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8772/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8864/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8962/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9059/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9154/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9253/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9351/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9450/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9546/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9642/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9737/10000 (97%)\n","\n","Train Epoch: 9  [0/60000 (0%)]\t1.537185\n","Train Epoch: 9  [2000/60000 (3%)]\t1.586708\n","Train Epoch: 9  [4000/60000 (7%)]\t1.587698\n","Train Epoch: 9  [6000/60000 (10%)]\t1.525575\n","Train Epoch: 9  [8000/60000 (13%)]\t1.499756\n","Train Epoch: 9  [10000/60000 (17%)]\t1.544183\n","Train Epoch: 9  [12000/60000 (20%)]\t1.487017\n","Train Epoch: 9  [14000/60000 (23%)]\t1.552737\n","Train Epoch: 9  [16000/60000 (27%)]\t1.569280\n","Train Epoch: 9  [18000/60000 (30%)]\t1.510259\n","Train Epoch: 9  [20000/60000 (33%)]\t1.532126\n","Train Epoch: 9  [22000/60000 (37%)]\t1.521221\n","Train Epoch: 9  [24000/60000 (40%)]\t1.536249\n","Train Epoch: 9  [26000/60000 (43%)]\t1.537140\n","Train Epoch: 9  [28000/60000 (47%)]\t1.530783\n","Train Epoch: 9  [30000/60000 (50%)]\t1.544475\n","Train Epoch: 9  [32000/60000 (53%)]\t1.544402\n","Train Epoch: 9  [34000/60000 (57%)]\t1.579795\n","Train Epoch: 9  [36000/60000 (60%)]\t1.513735\n","Train Epoch: 9  [38000/60000 (63%)]\t1.493551\n","Train Epoch: 9  [40000/60000 (67%)]\t1.534792\n","Train Epoch: 9  [42000/60000 (70%)]\t1.513997\n","Train Epoch: 9  [44000/60000 (73%)]\t1.513181\n","Train Epoch: 9  [46000/60000 (77%)]\t1.502140\n","Train Epoch: 9  [48000/60000 (80%)]\t1.495011\n","Train Epoch: 9  [50000/60000 (83%)]\t1.483149\n","Train Epoch: 9  [52000/60000 (87%)]\t1.499174\n","Train Epoch: 9  [54000/60000 (90%)]\t1.526195\n","Train Epoch: 9  [56000/60000 (93%)]\t1.509604\n","Train Epoch: 9  [58000/60000 (97%)]\t1.519823\n","\n","Test set: Average loss: 0.0001, Accuracy: 97/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 196/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 294/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 391/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 489/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 587/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 684/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 780/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 877/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 970/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1065/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1161/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1258/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1352/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1449/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1548/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1646/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1746/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1846/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1945/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2045/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2141/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2239/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2337/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2435/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2533/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2631/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2728/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2825/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2922/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3022/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3118/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3216/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3315/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3414/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3508/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 3602/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3699/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3798/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3896/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3993/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4090/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4187/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4287/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4382/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4478/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4572/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4669/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4768/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4861/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4961/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5057/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5155/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5255/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5355/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5452/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5546/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5642/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5740/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5838/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5936/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6033/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6127/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6222/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6319/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6416/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6514/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6611/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6710/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6807/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6905/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7003/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7098/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7196/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7295/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7393/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7491/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7589/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7685/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7782/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7880/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7976/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8075/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8172/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8270/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8369/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8466/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8565/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8660/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8758/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8855/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8954/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9052/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9148/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9245/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9341/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9436/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9535/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9633/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9729/10000 (97%)\n","\n","Train Epoch: 10  [0/60000 (0%)]\t1.519271\n","Train Epoch: 10  [2000/60000 (3%)]\t1.540794\n","Train Epoch: 10  [4000/60000 (7%)]\t1.517295\n","Train Epoch: 10  [6000/60000 (10%)]\t1.566341\n","Train Epoch: 10  [8000/60000 (13%)]\t1.530214\n","Train Epoch: 10  [10000/60000 (17%)]\t1.541764\n","Train Epoch: 10  [12000/60000 (20%)]\t1.492329\n","Train Epoch: 10  [14000/60000 (23%)]\t1.489548\n","Train Epoch: 10  [16000/60000 (27%)]\t1.536342\n","Train Epoch: 10  [18000/60000 (30%)]\t1.518049\n","Train Epoch: 10  [20000/60000 (33%)]\t1.535911\n","Train Epoch: 10  [22000/60000 (37%)]\t1.476309\n","Train Epoch: 10  [24000/60000 (40%)]\t1.529027\n","Train Epoch: 10  [26000/60000 (43%)]\t1.496846\n","Train Epoch: 10  [28000/60000 (47%)]\t1.501736\n","Train Epoch: 10  [30000/60000 (50%)]\t1.484972\n","Train Epoch: 10  [32000/60000 (53%)]\t1.516153\n","Train Epoch: 10  [34000/60000 (57%)]\t1.509679\n","Train Epoch: 10  [36000/60000 (60%)]\t1.541192\n","Train Epoch: 10  [38000/60000 (63%)]\t1.531112\n","Train Epoch: 10  [40000/60000 (67%)]\t1.512635\n","Train Epoch: 10  [42000/60000 (70%)]\t1.552402\n","Train Epoch: 10  [44000/60000 (73%)]\t1.510226\n","Train Epoch: 10  [46000/60000 (77%)]\t1.524838\n","Train Epoch: 10  [48000/60000 (80%)]\t1.536542\n","Train Epoch: 10  [50000/60000 (83%)]\t1.524343\n","Train Epoch: 10  [52000/60000 (87%)]\t1.524643\n","Train Epoch: 10  [54000/60000 (90%)]\t1.523335\n","Train Epoch: 10  [56000/60000 (93%)]\t1.503752\n","Train Epoch: 10  [58000/60000 (97%)]\t1.545226\n","\n","Test set: Average loss: 0.0001, Accuracy: 99/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 196/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 292/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 391/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 488/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 587/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 687/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 784/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 882/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 977/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1077/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1172/10000 (12%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1272/10000 (13%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1369/10000 (14%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1467/10000 (15%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1565/10000 (16%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1663/10000 (17%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1762/10000 (18%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 1860/10000 (19%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1956/10000 (20%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2054/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 2149/10000 (21%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2247/10000 (22%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2347/10000 (23%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2443/10000 (24%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2543/10000 (25%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2641/10000 (26%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2739/10000 (27%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2836/10000 (28%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 2934/10000 (29%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3031/10000 (30%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3128/10000 (31%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3225/10000 (32%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3324/10000 (33%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3422/10000 (34%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3521/10000 (35%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3618/10000 (36%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3718/10000 (37%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3815/10000 (38%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 3914/10000 (39%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4010/10000 (40%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4109/10000 (41%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4205/10000 (42%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4304/10000 (43%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4402/10000 (44%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4497/10000 (45%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4595/10000 (46%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4692/10000 (47%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4789/10000 (48%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 4887/10000 (49%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 4983/10000 (50%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5081/10000 (51%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5180/10000 (52%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5278/10000 (53%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5374/10000 (54%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5472/10000 (55%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5571/10000 (56%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5670/10000 (57%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 5766/10000 (58%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5866/10000 (59%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 5964/10000 (60%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6062/10000 (61%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6157/10000 (62%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6255/10000 (63%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 6351/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6449/10000 (64%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6549/10000 (65%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6647/10000 (66%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6746/10000 (67%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6844/10000 (68%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 6942/10000 (69%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 7036/10000 (70%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7133/10000 (71%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7233/10000 (72%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7330/10000 (73%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7427/10000 (74%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7525/10000 (75%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7624/10000 (76%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7723/10000 (77%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7820/10000 (78%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 7918/10000 (79%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8015/10000 (80%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8113/10000 (81%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8207/10000 (82%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8302/10000 (83%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8397/10000 (84%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8490/10000 (85%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8583/10000 (86%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8680/10000 (87%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8779/10000 (88%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 8877/10000 (89%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 8973/10000 (90%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9068/10000 (91%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9162/10000 (92%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9261/10000 (93%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9360/10000 (94%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9456/10000 (95%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9553/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 9648/10000 (96%)\n","\n","\n","Test set: Average loss: 0.0001, Accuracy: 9747/10000 (97%)\n","\n"]}]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1mURRDE6ehFg","executionInfo":{"status":"ok","timestamp":1730429513273,"user_tz":240,"elapsed":819,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"72dee809-e3be-45ea-a118-33e53492407b"},"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","model.eval()\n","\n","data, target = test_data[0]\n","\n","data = data.unsqueeze(0).to(device)\n","\n","prediction = model.predict(data).item()\n","\n","print(f'Prediction: {prediction}')\n","\n","# Display output\n","image = data.squeeze(0).squeeze(0).cpu().numpy()\n","plt.imshow(image, cmap='gray')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"yzhDjYYFctwL","executionInfo":{"status":"ok","timestamp":1730429556085,"user_tz":240,"elapsed":414,"user":{"displayName":"Jules Lenge","userId":"12690050732545170617"}},"outputId":"6a1b216d-84db-4d89-b4c0-de796d09bfb6"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction: 7\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"doqMmnq6g8oR"},"execution_count":null,"outputs":[]}]}